{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Database Schema Setup",
        "description": "Design and implement the PostgreSQL database schema with core entities (Fighter, Fight, FightStats, Event) as specified in the PRD.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Create a Supabase PostgreSQL database with the following tables:\n\n1. fighters table with columns:\n   - id (SERIAL, primary key)\n   - name (varchar)\n   - height_cm (float)\n   - weight_lbs (float)\n   - reach_inches (float)\n   - stance (varchar)\n   - birth_date (date)\n   - wins (integer)\n   - losses (integer)\n   - draws (integer)\n   - no_contests (integer)\n\n2. events table with columns:\n   - id (SERIAL, primary key)\n   - name (varchar)\n   - date (timestamp)\n   - city (varchar)\n   - country (varchar)\n   - venue (varchar)\n\n3. fights table with columns:\n   - id (SERIAL, primary key)\n   - event_id (integer, foreign key)\n   - fighter_a_id (integer, foreign key)\n   - fighter_b_id (integer, foreign key)\n   - winner_id (integer, foreign key, nullable)\n   - method (varchar)\n   - round (integer)\n   - time (time)\n   - weight_class (varchar)\n   - is_title_fight (boolean)\n\n4. fight_stats table with columns:\n   - id (SERIAL, primary key)\n   - fight_id (integer, foreign key)\n   - fighter_id (integer, foreign key)\n   - total_strikes (integer)\n   - total_takedowns (integer)\n   - total_control_time (interval)\n\n5. round_stats table with columns:\n   - id (SERIAL, primary key)\n   - fight_stats_id (integer, foreign key)\n   - round_number (integer)\n   - strikes_landed (integer)\n   - strikes_attempted (integer)\n   - takedowns (integer)\n   - control_time (interval)\n\nImplement using SQLAlchemy 2.0 ORM with appropriate relationships and indexes. Create Alembic migrations for version control of the schema.",
        "testStrategy": "1. Write unit tests for SQLAlchemy models to verify relationships and constraints\n2. Create test fixtures with sample data\n3. Test database migrations (up and down)\n4. Verify foreign key constraints and cascading deletes\n5. Benchmark query performance with test data\n6. Validate SERIAL primary key generation and constraints",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Data Scraping Pipeline",
        "description": "Enhance the existing Greko scraper to extract UFC fight data from UFCStats.com, including fighter profiles, event information, fight results, and round-by-round statistics.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "Extend and improve the existing Greko scraper API to efficiently collect UFC data. The current Greko implementation already contains significant data (744 events, 8287 fights, 4429 fighters, and 38958 fight statistics records) but has several limitations to address:\n\n1. Enhance Greko scraper modules for:\n   - Fighter profiles (physical attributes, record)\n   - Event listings (date, location, card details)\n   - Fight results (winners, methods, times)\n   - Round-by-round statistics\n\n2. Implement rate limiting and retry logic in Greko to avoid IP bans (currently missing):\n   - Random delays between requests (1-3 seconds)\n   - Exponential backoff for failed requests\n   - Rotate user agents\n\n3. Add robust error handling (currently crashes on failed requests):\n   - Graceful handling of HTTP errors\n   - Logging of failed requests\n   - Recovery mechanisms for partial scraping sessions\n\n4. Extend Greko's data transformation functions to normalize scraped data:\n   - Convert height from ft/in to cm\n   - Standardize weight class names\n   - Parse time strings to proper time objects\n\n5. Integrate data validation using Great Expectations (currently no quality checks):\n   - Validate data types and ranges\n   - Check for missing required fields\n   - Verify referential integrity\n\n6. Add incremental scraping logic to Greko (currently always scrapes everything):\n   - Track last scraped date\n   - Only fetch new or updated content\n\n7. Implement database integration (currently only outputs CSV):\n   - Connect to PostgreSQL database\n   - Create appropriate insert/update operations\n   - Maintain data consistency\n\n8. Create a scheduler using Airflow to run daily updates (currently manual execution only)\n\nStore raw scraped data in JSON format before processing to allow for reprocessing if needed.",
        "testStrategy": "1. Create mock responses from UFCStats.com for testing Greko enhancements\n2. Verify enhanced Greko parser accuracy with known sample pages\n3. Test improved rate limiting and retry logic\n4. Verify error handling with various failure scenarios\n5. Validate extended data transformation functions\n6. Test incremental scraping with modified content\n7. Verify database integration with test database\n8. Test scheduling functionality with compressed timeframes\n9. Verify error handling with malformed HTML\n10. End-to-end test with a small subset of real pages\n11. Compare results between original Greko scraper and enhanced version\n12. Validate data consistency between existing CSV data and newly scraped data",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze existing Greko scraper capabilities",
            "description": "Review the current Greko scraper codebase to identify strengths, limitations, and areas for enhancement",
            "status": "done",
            "dependencies": [],
            "details": "Analyze the existing Greko scraper that has already collected 744 events, 8287 fights, 4429 fighters, and 38958 fight statistics records. Document the current architecture, data flow, and specific limitations: 1) No rate limiting, 2) No error handling, 3) No data validation, 4) No incremental updates, 5) CSV-only output, and 6) Manual execution requirement.",
            "testStrategy": "Create a comprehensive report documenting findings and recommendations for improvements."
          },
          {
            "id": 2,
            "title": "Extend Greko scraper modules for UFC data",
            "description": "Enhance existing Greko modules or create new ones to handle fighter profiles, event listings, fight results, and round statistics",
            "status": "done",
            "dependencies": [],
            "details": "Build upon the existing Greko scraper modules to ensure comprehensive data collection while maintaining compatibility with the existing dataset of 744 events, 8287 fights, 4429 fighters, and 38958 fight statistics records.",
            "testStrategy": "Compare output with existing CSV data to ensure consistency and identify any gaps or improvements."
          },
          {
            "id": 3,
            "title": "Implement rate limiting and request handling",
            "description": "Enhance Greko's request handling with better rate limiting, backoff strategies, and user agent rotation",
            "status": "done",
            "dependencies": [],
            "details": "Rate limiting implemented in live_scraper.py with random 1-3 second delays between requests and proper user agent headers.",
            "testStrategy": "Test with mock responses to verify delays are working correctly and measure request patterns to ensure they don't trigger anti-scraping measures."
          },
          {
            "id": 4,
            "title": "Add robust error handling",
            "description": "Implement comprehensive error handling to prevent crashes on failed requests",
            "status": "done",
            "dependencies": [],
            "details": "Basic error handling implemented with try/catch blocks and logging throughout live_scraper.py and database_integration.py.",
            "testStrategy": "Simulate various error conditions (timeouts, 404s, 500s, malformed HTML) and verify the scraper continues functioning."
          },
          {
            "id": 5,
            "title": "Implement data validation with Great Expectations",
            "description": "Integrate Great Expectations with scraper output to ensure data quality and consistency",
            "status": "pending",
            "dependencies": [],
            "details": "Add data validation capabilities using Great Expectations to ensure data quality and consistency before storage. This is optional but recommended for production quality.",
            "testStrategy": "Create test cases with both valid and invalid data to verify validation rules are working correctly."
          },
          {
            "id": 6,
            "title": "Add incremental scraping functionality",
            "description": "Enhance scraper to support tracking of last scraped date and selective updates",
            "status": "done",
            "dependencies": [],
            "details": "Incremental scraping implemented via get_existing_events() method that checks database for existing event URLs before scraping.",
            "testStrategy": "Test with historical data to verify only new content is scraped, and validate that updates to existing content are properly detected."
          },
          {
            "id": 7,
            "title": "Implement database integration",
            "description": "Add PostgreSQL database integration to replace CSV-only output",
            "status": "done",
            "dependencies": [],
            "details": "Database integration implemented in database_integration.py with direct PostgreSQL storage using SQLAlchemy.",
            "testStrategy": "Test database connections, insertions, and updates with sample data to verify proper integration."
          },
          {
            "id": 8,
            "title": "Create scheduling for automated scraping",
            "description": "Develop scheduling system for regular runs of the scraper",
            "status": "done",
            "dependencies": [],
            "details": "Scheduler implemented in scheduler.py using schedule library for weekly automated runs (Sundays at 6 AM).",
            "testStrategy": "Test scheduling with compressed timeframes to verify automated execution and proper handling of dependencies between tasks."
          }
        ]
      },
      {
        "id": 3,
        "title": "ETL Pipeline for Historical Data",
        "description": "Transform and clean existing UFC data already in Supabase through 5 phases: FK resolution, quality cleanup, type parsing, new derived columns, and GitHub Actions automation. All work done in backend/scraper/ using raw sqlalchemy.text().",
        "status": "done",
        "dependencies": [
          1,
          2
        ],
        "priority": "high",
        "details": "Work directly against existing Supabase tables. No JSON files, no Airflow, no Great Expectations. Five phases:\n\n1. FK Resolution:\n   - Parse BOUT strings in fight_details using rapidfuzz to match fighter names and populate fighter_a_id and fighter_b_id (currently 100% NULL)\n   - Populate winner_id and loser_id FKs in fight_results using rapidfuzz name matching\n   - Populate fighter_id FK in fight_stats (currently 100% NULL)\n   - Scrape 8 career stat columns in fighter_tott that are missing\n\n2. Quality Cleanup:\n   - Replace '--' and '---' placeholder values with NULL across all relevant columns\n   - Strip trailing spaces from METHOD column\n   - Normalize WEIGHTCLASS values to a canonical set (e.g. 'Lightweight', 'Welterweight', etc.)\n\n3. Type Parsing:\n   - Parse 'X of Y' strike strings into integer pairs (landed, attempted)\n   - Parse CTRL time strings (e.g. '2:34') into total seconds as integer\n   - Parse physical attributes (height, weight, reach) from text to float values\n   - Calculate total_fight_time_seconds from round and time columns\n\n4. New Derived Columns:\n   - Add is_title_fight BOOLEAN column derived from WEIGHTCLASS or bout context\n   - Add fight_bonus STRING column for performance bonuses (POTN, FOTN, etc.)\n\n5. GitHub Actions Automation:\n   - Create backend/scraper/post_scrape_clean.py script that runs all cleanup phases\n   - Wire into GitHub Actions workflow to run automatically after each scrape\n   - Script must be idempotent (safe to rerun)\n\nAll database operations use raw sqlalchemy.text() queries. No ORM, no pandas required.",
        "testStrategy": "1. Unit test each parsing function (strike parsing, time parsing, physical attribute parsing) with edge cases and invalid inputs\n2. Test rapidfuzz FK resolution with known fighter name variations and verify match accuracy\n3. Verify quality cleanup replaces all '--'/'---' values and normalizes weightclass correctly\n4. Test idempotency by running post_scrape_clean.py multiple times and verifying no data corruption\n5. Validate FK resolution coverage (target: >95% of NULL FKs resolved)\n6. Spot-check derived columns (is_title_fight, fight_bonus) against known fight records\n7. Integration test the full post_scrape_clean.py script against a staging Supabase instance\n8. Verify GitHub Actions workflow triggers correctly after scrape runs",
        "subtasks": [
          {
            "id": 1,
            "title": "FK Resolution: Populate fighter_a_id and fighter_b_id in fight_details",
            "description": "Use rapidfuzz to parse BOUT strings in the fight_details table and match fighter names against the fighters table to populate the currently NULL fighter_a_id and fighter_b_id foreign key columns.",
            "dependencies": [],
            "details": "1. Query all rows from fight_details where fighter_a_id IS NULL or fighter_b_id IS NULL.\n2. For each row, parse the BOUT string (e.g. 'Fighter A vs. Fighter B') to extract two fighter name tokens.\n3. Load all fighter names and IDs from the fighters table into memory.\n4. Use rapidfuzz.process.extractOne with a score threshold (e.g. 85) to match each extracted name to a canonical fighter record.\n5. Log any unmatched or low-confidence matches for manual review.\n6. Execute UPDATE statements using sqlalchemy.text() to set fighter_a_id and fighter_b_id.\n7. Wrap updates in transactions; rollback on error.\n8. Acceptance criteria: fighter_a_id and fighter_b_id NULL rate reduced to <1% for parseable BOUT strings; all updates logged with match scores.\n<info added on 2026-02-21T15:32:42.451Z>\nCOMPLETED. Script: backend/scraper/populate_fighter_fks.py. Results: 8,482/8,533 fight_details rows now have both fighter_a_id and fighter_b_id (99.4%). 16,961 exact matches, 3 fuzzy matches (score_cutoff=88, WRatio). 51 unresolved rows are all scraper placeholder rows ('win vs. ' / 'draw vs. ') \u00e2\u20ac\u201d not real fights. Real fight coverage is 100%. rapidfuzz 3.14.3 installed. Unresolved names logged to backend/scraper/unresolved_fighter_names.log.\n</info added on 2026-02-21T15:32:42.451Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "FK Resolution: Populate winner_id and loser_id in fight_results",
            "description": "Use rapidfuzz name matching to resolve winner and loser names in fight_results to fighter IDs and populate the winner_id and loser_id foreign key columns.",
            "dependencies": [],
            "details": "1. Query all fight_results rows where winner_id IS NULL or loser_id IS NULL.\n2. Extract winner and loser name strings from existing text columns.\n3. Load fighters table into memory (id, name).\n4. Apply rapidfuzz.process.extractOne with threshold 85 to match each name.\n5. For ambiguous matches (score between 75-85), log for manual review rather than auto-updating.\n6. Execute batch UPDATE statements via sqlalchemy.text() to set winner_id and loser_id.\n7. Verify referential integrity after updates.\n8. Acceptance criteria: winner_id and loser_id populated for all rows with parseable name data; match confidence scores stored in a log file.\n<info added on 2026-02-21T16:46:34.352Z>\nCOMPLETED. Script: backend/scraper/populate_result_fks.py. Results: 8,482/8,482 rows populated (100%). Breakdown: W/L: 5,369 rows, L/W: 2,963 rows, NC/Draw: 150 rows. is_winner=TRUE set for decisive results, FALSE for NC/Draw outcomes. Schema fix required prior to population: fight_results.fighter_id and opponent_id columns were VARCHAR(6) and widened to VARCHAR(8) to match Greko IDs; fight_stats.fighter_id also widened to VARCHAR(8).\n</info added on 2026-02-21T16:46:34.352Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "FK Resolution: Populate fighter_id in fight_stats and scrape missing fighter_tott columns",
            "description": "Resolve the fighter_id FK in fight_stats (currently 100% NULL) using rapidfuzz matching, and scrape the 8 missing career stat columns in fighter_tott from UFCStats.com.",
            "dependencies": [],
            "details": "1. For fight_stats FK resolution: query rows where fighter_id IS NULL, extract fighter name text, match against fighters table using rapidfuzz, and UPDATE via sqlalchemy.text().\n2. For fighter_tott: identify the 8 missing career stat columns (e.g. sig_str_landed_per_min, sig_str_absorbed_per_min, takedown_avg, submission_avg, sig_str_defense, takedown_defense, knockdown_avg, avg_fight_time).\n3. Scrape each fighter's stats page on UFCStats.com using requests/BeautifulSoup.\n4. Parse the 8 stat values and INSERT/UPDATE into fighter_tott using sqlalchemy.text().\n5. Implement rate limiting (1 req/sec) and retry logic.\n6. Acceptance criteria: fight_stats.fighter_id NULL rate <1%; all 8 fighter_tott columns populated for fighters with UFCStats profiles.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Quality Cleanup: Replace placeholder values, strip whitespace, and normalize WEIGHTCLASS",
            "description": "Perform data quality cleanup across all relevant tables: replace '--' and '---' with NULL, strip trailing spaces from METHOD, and normalize WEIGHTCLASS to a canonical set of values.",
            "dependencies": [],
            "details": "1. Identify all columns across fight_details, fight_results, fight_stats, and fighter_tott that contain '--' or '---' placeholder strings.\n2. Generate and execute UPDATE statements using sqlalchemy.text() to SET col = NULL WHERE col IN ('--', '---') for each identified column.\n3. Strip trailing (and leading) spaces from the METHOD column in fight_results: UPDATE fight_results SET method = TRIM(method).\n4. Define canonical WEIGHTCLASS values: ['Heavyweight', 'Light Heavyweight', 'Middleweight', 'Welterweight', 'Lightweight', 'Featherweight', 'Bantamweight', 'Flyweight', \"Women's Strawweight\", \"Women's Flyweight\", \"Women's Bantamweight\", \"Women's Featherweight\", 'Catch Weight', 'Open Weight'].\n5. Build a mapping of known variants to canonical values and apply via UPDATE.\n6. Log any unmapped WEIGHTCLASS values for manual review.\n7. Acceptance criteria: zero '--'/'---' values remain; METHOD has no trailing spaces; WEIGHTCLASS values match canonical set or are NULL.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Type Parsing: Parse strike strings, CTRL time, physical attributes, and fight time",
            "description": "Implement parsing functions to convert text-encoded numeric data into proper integer/float types: 'X of Y' strike strings, CTRL time strings, physical attributes, and total fight time seconds.",
            "dependencies": [],
            "details": "1. Strike parsing: write parse_strikes(val) -> (int, int) that splits 'X of Y' into (landed, attempted); handle NULL/invalid input gracefully.\n2. Apply to all strike columns in fight_stats, adding paired _landed and _attempted integer columns via ALTER TABLE and UPDATE using sqlalchemy.text().\n3. CTRL time parsing: write parse_ctrl_time(val) -> int that converts 'M:SS' to total seconds; UPDATE ctrl_time_seconds column.\n4. Physical attributes: write parse_height(val) -> float (cm), parse_weight(val) -> float (lbs), parse_reach(val) -> float (inches) for fighters table; UPDATE height_cm, weight_lbs, reach_inches.\n5. Fight time: calculate total_fight_time_seconds from round number and time-in-round columns using formula: (round-1)*300 + parse_ctrl_time(time); UPDATE fight_results.\n6. All operations use sqlalchemy.text(); functions are unit-testable in isolation.\n7. Acceptance criteria: all parsed columns contain correct integer/float values; original text columns preserved; edge cases (NULL, '--', malformed) return NULL without crashing.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "New Derived Columns: Add is_title_fight BOOLEAN and fight_bonus STRING",
            "description": "Add and populate two new derived columns: is_title_fight (BOOLEAN) derived from WEIGHTCLASS or bout context, and fight_bonus (STRING) for performance bonuses like POTN and FOTN.",
            "dependencies": [],
            "details": "1. Add is_title_fight column: ALTER TABLE fight_results ADD COLUMN IF NOT EXISTS is_title_fight BOOLEAN DEFAULT FALSE using sqlalchemy.text().\n2. Derive is_title_fight by checking if WEIGHTCLASS contains 'Title' or if BOUT string contains 'Championship'/'Title'; UPDATE accordingly.\n3. Add fight_bonus column: ALTER TABLE fight_results ADD COLUMN IF NOT EXISTS fight_bonus VARCHAR(50).\n4. Parse bonus indicators from available text fields; map to canonical values: 'POTN' (Performance of the Night), 'FOTN' (Fight of the Night), 'KOTN' (KO of the Night), 'SOTN' (Submission of the Night).\n5. A fight may have multiple bonuses; store as comma-separated string or use the primary bonus.\n6. All DDL and DML via sqlalchemy.text().\n7. Acceptance criteria: is_title_fight correctly set for known title fights (spot-check 20 records); fight_bonus populated for events known to award bonuses; columns are idempotent on re-run (IF NOT EXISTS guards).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Unit Tests for All Parsing and Matching Functions",
            "description": "Write comprehensive unit tests for all parsing functions (strike, time, physical attributes, fight time) and rapidfuzz FK resolution logic with edge cases and invalid inputs.",
            "dependencies": [],
            "details": "1. Create backend/scraper/tests/test_parsers.py covering:\n   - parse_strikes: valid 'X of Y', '0 of 0', NULL, '--', malformed strings\n   - parse_ctrl_time: '0:00', '5:00', '2:34', NULL, '--', invalid format\n   - parse_height/weight/reach: various text formats, NULL, '--'\n   - total_fight_time_seconds: all 5 rounds, partial rounds, NULL inputs\n2. Create backend/scraper/tests/test_fk_resolution.py covering:\n   - Exact name matches\n   - Name variations (nicknames, abbreviations)\n   - Low-confidence matches that should be skipped\n   - Empty/NULL name inputs\n3. Use pytest; mock database calls with unittest.mock.\n4. Achieve >90% line coverage on all parsing modules.\n5. Acceptance criteria: all tests pass; edge cases documented; CI runs tests on push.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Build post_scrape_clean.py Orchestration Script",
            "description": "Create the backend/scraper/post_scrape_clean.py script that orchestrates all five ETL phases in sequence, is fully idempotent, and provides structured logging and error reporting.",
            "dependencies": [],
            "details": "1. Create backend/scraper/post_scrape_clean.py as the single entry point for all cleanup phases.\n2. Structure as sequential phase runner: Phase 1 (FK Resolution) -> Phase 2 (Quality Cleanup) -> Phase 3 (Type Parsing) -> Phase 4 (Derived Columns).\n3. Each phase wrapped in try/except with transaction rollback on failure; failed phase logs error and continues or halts based on severity flag.\n4. Idempotency: all UPDATE/ALTER statements use IF NOT EXISTS, WHERE col IS NULL guards, or UPSERT patterns so re-running produces no duplicate work.\n5. Accept --phase argument to run individual phases for debugging.\n6. Use Python logging module; output structured logs with phase name, rows affected, duration, and error details.\n7. Load DB connection string from environment variable SUPABASE_DB_URL.\n8. Acceptance criteria: script runs end-to-end without error on clean and already-processed data; --phase flag works; all phases complete in <30 minutes on full dataset.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "GitHub Actions Workflow for Automated Post-Scrape Cleanup",
            "description": "Wire post_scrape_clean.py into a GitHub Actions workflow that triggers automatically after each scrape run, with proper secret management and failure notifications.",
            "dependencies": [],
            "details": "1. Create .github/workflows/post_scrape_clean.yml.\n2. Trigger: workflow_run with workflows: ['Scrape UFC Data'] and types: [completed]; also support manual workflow_dispatch trigger.\n3. Job steps:\n   a. Checkout repo\n   b. Set up Python 3.11\n   c. Install dependencies from backend/scraper/requirements.txt\n   d. Run python backend/scraper/post_scrape_clean.py with SUPABASE_DB_URL from GitHub Secrets\n4. Add conditional: only run if triggering workflow succeeded (github.event.workflow_run.conclusion == 'success').\n5. On failure: send notification via GitHub Actions built-in failure reporting; optionally post to Slack via webhook secret.\n6. Cache pip dependencies for faster runs.\n7. Acceptance criteria: workflow triggers correctly after scrape; SUPABASE_DB_URL secret used (never logged); workflow passes on successful scrape; skips on failed scrape; manual trigger works.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "End-to-End Validation and Data Quality Report",
            "description": "Implement a validation script that runs post-cleanup to verify data quality metrics, FK integrity, and column completeness, producing a summary report for each pipeline run.",
            "dependencies": [],
            "details": "1. Create backend/scraper/validate_etl.py that runs after post_scrape_clean.py.\n2. Validation checks using sqlalchemy.text() queries:\n   - FK completeness: % of fight_details rows with non-NULL fighter_a_id/fighter_b_id\n   - FK completeness: % of fight_results rows with non-NULL winner_id/loser_id\n   - FK completeness: % of fight_stats rows with non-NULL fighter_id\n   - Zero '--'/'---' values across all cleaned columns\n   - WEIGHTCLASS values all in canonical set\n   - Strike columns: % non-NULL for _landed/_attempted pairs\n   - is_title_fight: non-NULL rate\n   - fight_bonus: distribution of bonus types\n3. Output a JSON summary report to backend/scraper/reports/etl_validation_{timestamp}.json.\n4. Define pass/fail thresholds (e.g. FK completeness >95% = pass).\n5. Exit with non-zero code if any threshold fails (causes GitHub Actions job to fail).\n6. Integrate validate_etl.py call at end of post_scrape_clean.py and in GitHub Actions workflow.\n7. Acceptance criteria: validation report generated on every run; pipeline fails loudly if quality thresholds not met; report archived as GitHub Actions artifact.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "FastAPI Backend Setup",
        "description": "Initialize the FastAPI project structure with middleware, error handling, logging, and basic health check endpoints. Uses raw SQL with sqlalchemy.text() and Pydantic schemas for serialization (no ORM models). Public platform with no authentication required.",
        "status": "in-progress",
        "dependencies": [
          1,
          3
        ],
        "priority": "high",
        "details": "Set up a FastAPI application with the following components:\n\n1. Project structure:\n   ```\n   backend/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac api/\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac v1/\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac endpoints/\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac predictions.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac analytics.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac fighters.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac fights.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac events.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac router.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac dependencies.py\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac core/\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac config.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac logging.py\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac db/\n     \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac database.py  (SessionLocal, engine already configured - use as-is)\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac schemas/\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac fighter.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac fight.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac event.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac prediction.py\n     \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac analytics.py\n     \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac main.py\n   ```\n\n2. Key API endpoints to implement:\n   - `GET /fighters` - list all fighters\n   - `GET /fighters/{id}` - single fighter detail\n   - `GET /fights` - list all fights\n   - `GET /fights/{id}` - single fight detail\n   - `GET /events` - list all events\n   - `GET /events/{id}` - single event detail\n   - `POST /predictions/fight-outcome` - predict fight outcome\n   - `GET /analytics/style-evolution` - style evolution analytics\n   - `GET /analytics/fighter-endurance/{id}` - fighter endurance analytics\n\n3. Database access pattern:\n   - Import SessionLocal and engine from `backend/db/database.py`\n   - Use `sqlalchemy.text()` for all queries (no ORM models)\n   - Use Pydantic schemas for request/response serialization\n   - Example pattern:\n     ```python\n     from backend.db.database import SessionLocal\n     from sqlalchemy import text\n     \n     def get_fighters(db: Session):\n         result = db.execute(text(\"SELECT * FROM fighters\"))\n         return result.mappings().all()\n     ```\n\n4. Middleware configuration:\n   - CORS with appropriate origins (allow frontend dev server)\n   - Request ID generation\n   - Timing middleware\n   - Error handling middleware\n   - No authentication middleware needed (public platform)\n\n5. Pydantic schemas (no ORM, pure serialization):\n   - FighterBase, FighterResponse\n   - FightBase, FightResponse\n   - EventBase, EventResponse\n   - PredictionRequest, PredictionResponse\n   - AnalyticsResponse\n\n6. Health check endpoints:\n   - `/health` for basic API health\n   - `/health/db` for database connectivity (run a simple `SELECT 1` via sqlalchemy.text())\n\n7. Logging configuration:\n   - Structured JSON logging\n   - Log rotation\n   - Different log levels for environments\n\nImplement using Uvicorn for development and Gunicorn for production with appropriate worker configurations. No Redis, no security/auth module.",
        "testStrategy": "1. Unit test middleware functions\n2. Test health check endpoints (`/health`, `/health/db`)\n3. Verify CORS configuration\n4. Test error handling with forced exceptions\n5. Benchmark request processing time\n6. Validate OpenAPI schema generation\n7. Test logging output format and content\n8. Test each endpoint with sample data: GET /fighters, GET /fighters/{id}, GET /fights, GET /fights/{id}, GET /events, GET /events/{id}\n9. Test POST /predictions/fight-outcome with valid and invalid payloads\n10. Test analytics endpoints return correctly structured responses\n11. Verify raw SQL queries via sqlalchemy.text() return correct Pydantic-serializable data",
        "subtasks": [
          {
            "id": 1,
            "title": "Project Structure and Core Configuration",
            "description": "Initialize the FastAPI project directory structure, core configuration module, and logging setup as the foundation for all other components.",
            "dependencies": [],
            "details": "Create the full directory tree under backend/ as specified: api/v1/endpoints/, core/, db/, schemas/, and main.py. Implement backend/core/config.py using Pydantic BaseSettings to load environment variables (DATABASE_URL, ENVIRONMENT, LOG_LEVEL, ALLOWED_ORIGINS, etc.). Implement backend/core/logging.py with structured JSON logging using python-json-logger, log rotation via logging.handlers.RotatingFileHandler, and environment-aware log levels (DEBUG for dev, INFO/WARNING for prod). Create empty __init__.py files in all packages. Acceptance criteria: All directories and files exist, config loads from .env without errors, logging outputs valid JSON to console and file, log rotation is configured.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Database Session Management and Dependencies",
            "description": "Set up the database session dependency for FastAPI endpoints, importing the existing SessionLocal and engine from backend/db/database.py and providing a reusable get_db dependency.",
            "dependencies": [],
            "details": "In backend/api/dependencies.py, implement a get_db() generator function that yields a SessionLocal instance and ensures the session is closed in a finally block. Verify the import from backend/db/database.py works correctly (SessionLocal, engine already configured). Add a utility function to test DB connectivity by executing sqlalchemy.text('SELECT 1'). Do NOT redefine engine or SessionLocal. Acceptance criteria: get_db() yields a valid session, session is always closed after request, connectivity test function returns True on success and raises a clear exception on failure.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Pydantic Schemas Definition",
            "description": "Define all Pydantic schemas for request/response serialization across fighters, fights, events, predictions, and analytics \u00e2\u20ac\u201d with no ORM models.",
            "dependencies": [],
            "details": "Create the following schema files under backend/schemas/: fighter.py (FighterBase, FighterResponse with id, name, height_cm, weight_lbs, reach_inches, stance, birth_date, wins, losses, draws, no_contests), fight.py (FightBase, FightResponse with relevant fight fields), event.py (EventBase, EventResponse with id, name, date, city, country), prediction.py (PredictionRequest with fighter_a_id, fighter_b_id and optional parameters; PredictionResponse with win_probability, predicted_winner, confidence, method_probabilities), analytics.py (StyleEvolutionResponse, FighterEnduranceResponse). All schemas use Pydantic v2 with model_config = ConfigDict(from_attributes=False) since there are no ORM models. Acceptance criteria: All schemas import without errors, PredictionRequest validates required fields, all Response schemas serialize dict/mapping inputs correctly.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Middleware and Error Handling Configuration",
            "description": "Configure CORS, request ID generation, timing middleware, and global error handling in main.py to make the API production-ready.",
            "dependencies": [],
            "details": "In backend/main.py, create the FastAPI app instance and add the following middleware in order: (1) CORSMiddleware with origins from config (e.g., http://localhost:3000, http://localhost:5173), allow_methods=['*'], allow_headers=['*']; (2) Custom RequestIDMiddleware that generates a UUID per request and attaches it to request.state and response headers as X-Request-ID; (3) Custom TimingMiddleware that measures request duration and logs it with the request ID and path; (4) Global exception handler using @app.exception_handler(Exception) that returns a structured JSON error response with request_id, message, and status_code, and logs the full traceback. No authentication middleware. Acceptance criteria: CORS headers present on responses, X-Request-ID header in every response, timing logged per request, unhandled exceptions return JSON (not HTML), 404s return structured JSON.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "API Endpoints Implementation and Router Setup",
            "description": "Implement all v1 API endpoint modules for fighters, fights, events, predictions, and analytics using raw SQL via sqlalchemy.text(), and wire them into the versioned router.",
            "dependencies": [],
            "details": "Implement each endpoint file under backend/api/v1/endpoints/: fighters.py (GET /fighters with optional pagination query params, GET /fighters/{id}), fights.py (GET /fights with optional filters, GET /fights/{id}), events.py (GET /events, GET /events/{id}), predictions.py (POST /predictions/fight-outcome \u00e2\u20ac\u201d accepts PredictionRequest, queries fighter stats via raw SQL, returns PredictionResponse with placeholder logic until ML model is ready in Task 6), analytics.py (GET /analytics/style-evolution with fighter_id query param, GET /analytics/fighter-endurance/{id}). All DB queries use db.execute(text('...')).mappings().all() or .first(). Return 404 HTTPException when single resources are not found. In backend/api/v1/router.py, create an APIRouter and include all endpoint routers with appropriate prefixes and tags. In main.py, include the v1 router under prefix /api/v1. Acceptance criteria: All endpoints return correct HTTP status codes, 404 on missing resources, responses validate against Pydantic schemas, raw SQL used exclusively (no ORM), OpenAPI docs auto-generated at /docs.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Health Check Endpoints and Server Configuration",
            "description": "Implement /health and /health/db endpoints, finalize main.py app wiring, and configure Uvicorn/Gunicorn server settings for development and production.",
            "dependencies": [],
            "details": "Add health check routes directly in main.py or a dedicated health router: GET /health returns {status: 'ok', environment: str, version: str, timestamp: ISO datetime}; GET /health/db executes db.execute(text('SELECT 1')) via a direct SessionLocal() call (not the request-scoped dependency) and returns {status: 'ok', db: 'connected'} or {status: 'error', db: str(exception)} with HTTP 503 on failure. Create a backend/run_dev.py or document uvicorn command: uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000. Create a Procfile or gunicorn.conf.py for production: gunicorn backend.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000 --access-logfile - --error-logfile -. Ensure app startup event logs the environment and config summary. Acceptance criteria: /health returns 200 with valid JSON, /health/db returns 200 when DB is reachable and 503 when not, Uvicorn starts successfully in dev mode, Gunicorn config file is valid, startup log message appears on launch.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Feature Engineering Pipeline",
        "description": "Develop a feature engineering pipeline to process raw UFC data into ML-ready features, including fighter differentials, rolling performance metrics, and style classifications.",
        "details": "Create a comprehensive feature engineering pipeline using pandas and numpy:\n\n1. Fighter differential features:\n   - Height differential (cm)\n   - Weight differential (lbs)\n   - Reach differential (inches)\n   - Age differential (years)\n   - Experience differential (fights)\n   - Win streak differential\n\n2. Rolling performance metrics:\n   - 3-fight rolling average for strikes landed/minute\n   - 3-fight rolling average for takedown accuracy\n   - 3-fight rolling average for submission attempts\n   - Exponentially weighted averages for recent performance\n\n3. Style classification features:\n   - Striking vs. grappling ratio\n   - Aggression score (forward pressure)\n   - Defensive metrics (strikes absorbed, takedown defense)\n   - Finishing ability (KO/submission percentage)\n\n4. Time-based features:\n   - Days since last fight\n   - Career length in days\n   - Age at fight time\n   - Time in specific weight class\n\n5. Opponent quality metrics:\n   - Opponent win percentage\n   - Strength of schedule\n   - Common opponents analysis\n\nImplement feature selection using mutual information and correlation analysis. Create a pipeline that can be used both for batch processing historical data and real-time feature generation for predictions.",
        "testStrategy": "1. Unit test each feature calculation function\n2. Verify feature distributions and ranges\n3. Test with edge cases (debut fighters, long layoffs)\n4. Validate feature importance with preliminary models\n5. Benchmark processing time for large datasets\n6. Test reproducibility with fixed random seeds\n7. Verify handling of missing data",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "ML Model Training and Serving",
        "description": "Implement the machine learning pipeline to train fight prediction models (win/loss and method prediction) and create a model serving infrastructure using joblib serialization, loaded at API startup.",
        "status": "pending",
        "dependencies": [
          4,
          5
        ],
        "priority": "high",
        "details": "Build a complete ML pipeline using scikit-learn 1.3+ and XGBoost 2.0:\n\n1. Data preparation:\n   - Train/validation/test splitting (temporal split)\n   - Feature scaling and normalization\n   - Class imbalance handling (SMOTE or class weights)\n\n2. Model training:\n   - Binary classification model for win/loss prediction\n     - XGBoost with hyperparameter tuning\n     - Cross-validation with time-based splits\n   - Multi-class model for fight outcome method\n     - Random Forest for KO/TKO vs Submission vs Decision\n     - Calibrated probabilities\n\n3. Model evaluation:\n   - Accuracy, precision, recall metrics\n   - ROC-AUC and PR-AUC curves\n   - Confusion matrices\n   - Feature importance analysis (critical for UI sliders feature)\n   - Backtesting on historical fights\n\n4. Model serialization:\n   - Use joblib to serialize trained models to backend/models/ directory\n   - Save feature scaler alongside models\n   - Store feature importance data as JSON for frontend consumption\n   - Version models with simple filename conventions (e.g., win_loss_v1.joblib, method_v1.joblib)\n\n5. Model serving:\n   - Models loaded at FastAPI application startup (Task 4 lifespan event)\n   - FastAPI endpoints for predictions in backend/api/v1/endpoints/predictions.py\n   - Input validation with Pydantic\n   - Batch prediction capability\n\n6. Interpretability (priority for UI):\n   - Export feature importance rankings from XGBoost and Random Forest\n   - Generate feature importance charts/data consumable by the frontend sliders feature\n   - SHAP values for individual prediction explanations (optional but valuable)\n\nImplement a simple model loader utility that reads joblib files from backend/models/ at startup and exposes them via FastAPI app state.",
        "testStrategy": "1. Test model accuracy on holdout data\n2. Verify calibration of probability outputs\n3. Benchmark prediction latency\n4. Test model loading from backend/models/ directory at startup\n5. Verify joblib serialization/deserialization round-trip\n6. Test concurrent prediction requests\n7. Verify model reproducibility with fixed random seeds\n8. Test feature importance output format matches frontend expectations\n9. Validate batch prediction endpoint",
        "subtasks": [
          {
            "id": 1,
            "title": "Data preparation and train/test splitting",
            "description": "Load engineered features from Task 5, perform temporal train/validation/test split, apply feature scaling, and handle class imbalance using SMOTE or class weights.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Train XGBoost win/loss binary classifier",
            "description": "Train an XGBoost binary classification model for win/loss prediction with hyperparameter tuning and time-based cross-validation. Evaluate with accuracy, ROC-AUC, and PR-AUC metrics.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Train Random Forest method multi-class classifier",
            "description": "Train a Random Forest multi-class model to predict fight outcome method (KO/TKO vs Submission vs Decision) with calibrated probabilities. Evaluate with confusion matrix and per-class metrics.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Feature importance extraction and export",
            "description": "Extract feature importance rankings from both XGBoost and Random Forest models. Export as JSON files to backend/models/ for frontend consumption. This data powers the UI sliders feature showing which attributes matter most.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Serialize models with joblib to backend/models/",
            "description": "Serialize trained models and feature scalers using joblib to the backend/models/ directory. Use clear filename conventions (e.g., win_loss_v1.joblib, method_v1.joblib, scaler_v1.joblib). Include a model manifest JSON with metadata.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Model loader utility and FastAPI startup integration",
            "description": "Create a model loader utility (backend/ml/model_loader.py) that reads joblib files from backend/models/ at application startup via FastAPI lifespan event. Expose loaded models through app state for use in prediction endpoints.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Prediction endpoints implementation",
            "description": "Implement FastAPI prediction endpoints in backend/api/v1/endpoints/predictions.py. Include single fight prediction and batch prediction capability. Use Pydantic for input validation. Return probabilities and feature importance context.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Model evaluation and backtesting report",
            "description": "Run full evaluation on holdout test set: accuracy, precision, recall, ROC-AUC, PR-AUC, confusion matrices, and backtesting on historical fights. Save evaluation report to backend/models/evaluation_report.json.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "React Frontend Foundation",
        "description": "Set up the React frontend application with TypeScript, Vite, Tailwind CSS, routing, and state management according to the technical architecture specified in the PRD. Includes foundation for five frontend views: three ML dashboards (predictions, style evolution, endurance) and two data views (Recent Events, Fighter/Database Lookup).",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Initialize and configure a React 18 frontend application:\n\n1. Project setup:\n   - Initialize with Vite and TypeScript 4.9+\n   - Configure ESLint and Prettier\n   - Set up directory structure\n   ```\n   src/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac assets/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac components/\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac common/\n     \u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac layout/\n     \u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac features/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac hooks/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac pages/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac services/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac store/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac types/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac utils/\n     \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac App.tsx\n     \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac main.tsx\n   ```\n\n2. Styling setup:\n   - Configure Tailwind CSS 3.0\n   - Create design tokens (colors, spacing, etc.)\n   - Set up dark mode support\n   - Create responsive breakpoints (320px, 768px, 1024px, 1440px)\n\n3. Routing configuration:\n   - Set up React Router v6\n   - Create route definitions\n   - Implement lazy loading for routes\n   - Add route guards/protection\n\n4. State management:\n   - Configure React Context API\n   - Set up useReducer for global state\n   - Create typed actions and reducers\n   - Implement persistence with localStorage\n\n5. API integration:\n   - Set up Axios with interceptors\n   - Create API service classes\n   - Implement request/response types\n   - Add error handling\n\n6. Common components:\n   - Button, Input, Card components\n   - Loading indicators\n   - Error boundaries\n   - Toast notifications\n\n7. Testing setup:\n   - Configure Jest and React Testing Library\n   - Set up test utilities\n   - Create mock providers\n\n8. Additional frontend views (no ML required):\n   a. Recent Events View:\n      - Displays the last ~10 UFC events with card results (winner, method, round)\n      - Queries event_details joined with fight_results ordered by date_proper DESC\n      - Consumes GET /events endpoint\n   b. Fighter/Database Lookup View:\n      - Search bar for fighter name\n      - Shows fighter profile with physical stats from fighter_tott\n      - Full fight history with methods and rounds from fight_results\n      - Consumes GET /fighters and GET /fighters/{id} endpoints\n      - IMPORTANT: Display a prominent note 'Detailed round stats available from 2015+' on fighter profiles, as fight_stats (round-by-round detail) is only reliably available from 2015 onwards\n\nImplement responsive layouts with mobile-first approach and ensure accessibility compliance.",
        "testStrategy": "1. Test component rendering\n2. Verify responsive breakpoints\n3. Test routing functionality\n4. Validate state management\n5. Test API service mocks\n6. Verify dark mode toggle\n7. Test accessibility with axe-core\n8. Validate build output and bundle size\n9. Test Recent Events view renders event cards with correct winner/method/round data\n10. Test Fighter Lookup search functionality and profile display\n11. Verify 'Detailed round stats available from 2015+' caveat note appears on fighter profiles\n12. Validate that fight history displays correctly for fighters with pre-2015 records",
        "subtasks": [
          {
            "id": 1,
            "title": "Project Initialization with Vite, TypeScript, and Tooling",
            "description": "Initialize the React 18 frontend project using Vite with TypeScript 4.9+, configure ESLint and Prettier, and establish the complete directory structure as specified in the PRD.",
            "status": "pending",
            "dependencies": [],
            "details": "1. Run `npm create vite@latest` with React + TypeScript template\n2. Configure tsconfig.json with strict mode, path aliases (e.g., @/components, @/hooks, @/services)\n3. Install and configure ESLint with @typescript-eslint, eslint-plugin-react, eslint-plugin-react-hooks\n4. Install and configure Prettier with .prettierrc and .prettierignore\n5. Create the full directory structure: src/assets/, src/components/common/, src/components/layout/, src/components/features/, src/hooks/, src/pages/, src/services/, src/store/, src/types/, src/utils/\n6. Create placeholder index files in each directory\n7. Configure Vite with path aliases matching tsconfig\n8. Set up .env files (.env, .env.development, .env.production) with VITE_ prefixed variables\n9. Configure Jest and React Testing Library: install jest, @testing-library/react, @testing-library/jest-dom, jest-environment-jsdom\n10. Create jest.config.ts, setupTests.ts, and a test utilities file with mock providers\n11. Add npm scripts: dev, build, preview, lint, format, test\n\nAcceptance Criteria:\n- `npm run dev` starts the dev server without errors\n- `npm run build` produces a valid dist/ output\n- `npm run lint` and `npm run format` execute without errors\n- `npm run test` runs successfully with a sample test\n- All directories exist with placeholder files\n- TypeScript strict mode enabled with no type errors",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Tailwind CSS Styling Configuration with Design Tokens and Dark Mode",
            "description": "Install and configure Tailwind CSS 3.0 with custom design tokens, dark mode support, and responsive breakpoints aligned with the PRD specifications.",
            "status": "pending",
            "dependencies": [],
            "details": "1. Install tailwindcss, postcss, autoprefixer and run `npx tailwindcss init -p`\n2. Configure tailwind.config.ts with content paths covering all src files\n3. Define custom design tokens in the theme.extend section:\n   - Colors: primary, secondary, accent, neutral, success, warning, error palettes\n   - Typography: font families, sizes, weights\n   - Spacing: custom spacing scale\n   - Border radius, shadows, z-index values\n4. Configure responsive breakpoints: sm: 320px, md: 768px, lg: 1024px, xl: 1440px\n5. Enable dark mode with class strategy: `darkMode: 'class'`\n6. Create src/styles/globals.css with @tailwind directives and CSS custom properties for design tokens\n7. Create src/styles/tokens.ts exporting design token constants for use in TypeScript\n8. Import globals.css in main.tsx\n9. Create a ThemeProvider component in src/components/layout/ that manages dark/light mode toggle using localStorage persistence\n10. Add a useDarkMode custom hook in src/hooks/\n11. Verify Tailwind purging works correctly in production build\n\nAcceptance Criteria:\n- Tailwind utility classes apply correctly in components\n- Dark mode toggles via class on html element and persists across page reloads\n- All four responsive breakpoints work as expected\n- Design tokens are accessible both as Tailwind classes and TypeScript constants\n- Production build CSS is properly purged and optimized",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "React Router v6 Configuration with Lazy Loading and Route Guards",
            "description": "Set up React Router v6 with all application route definitions including Recent Events and Fighter Lookup views, lazy loading for code splitting, and route guard components for protected routes.",
            "status": "pending",
            "dependencies": [],
            "details": "1. Install react-router-dom v6\n2. Create src/router/index.tsx with centralized route definitions using createBrowserRouter\n3. Define routes for all planned pages:\n   - / (Home/Dashboard)\n   - /fighters (Fighter/Database Lookup view \u00e2\u20ac\u201d search bar + fighter profiles)\n   - /fighters/:id (Fighter detail with profile, physical stats, fight history)\n   - /events (Recent Events view \u00e2\u20ac\u201d last ~10 UFC events with card results)\n   - /events/:id (Event detail)\n   - /analytics (Analytics dashboard)\n   - /analytics/style-evolution (Style Evolution Timeline - Task 9)\n   - /analytics/endurance (Fighter Endurance Dashboard - Task 10)\n   - /predictions (Predictions page)\n   - /* (404 Not Found)\n4. Implement lazy loading for all page components using React.lazy() and Suspense with a loading fallback\n5. Create a RouteGuard component in src/components/common/ for future protected routes\n6. Create a Layout component in src/components/layout/ with Header, Sidebar/Nav, and Footer slots\n7. Create placeholder page components in src/pages/ for each route, including RecentEventsPage and FighterLookupPage\n8. Add a NotFound (404) page component\n9. Create a LoadingSpinner component used as Suspense fallback\n10. Configure RouterProvider in App.tsx\n11. Write tests for routing behavior and lazy loading\n\nAcceptance Criteria:\n- All defined routes render their respective page components\n- Lazy loading splits code into separate chunks (verify in build output)\n- Suspense fallback displays during chunk loading\n- 404 page renders for unknown routes\n- Navigation between routes works without full page reload\n- RouteGuard component correctly handles access control logic\n- /events and /fighters routes have placeholder pages ready for implementation in subtask 6",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "State Management with Context API, useReducer, and Persistence",
            "description": "Implement global state management using React Context API with useReducer, typed actions and reducers, and localStorage persistence for relevant state slices.",
            "status": "pending",
            "dependencies": [],
            "details": "1. Create src/store/index.ts as the main store entry point\n2. Define TypeScript interfaces for global state in src/types/store.ts:\n   - AppState (theme, notifications, ui state)\n   - FilterState (active filters for fighters/events/analytics)\n   - UserPreferencesState (dark mode, layout preferences)\n3. Create typed action types and action creators in src/store/actions.ts\n4. Implement reducers in src/store/reducers/:\n   - appReducer.ts\n   - filterReducer.ts\n   - userPreferencesReducer.ts\n5. Create a root reducer combining all reducers\n6. Create AppContext and AppProvider in src/store/AppContext.tsx using useReducer\n7. Implement localStorage persistence middleware pattern:\n   - Create src/utils/localStorage.ts with typed get/set/remove helpers\n   - Persist userPreferences and filterState to localStorage\n   - Rehydrate state from localStorage on app initialization\n8. Create custom hooks for consuming store slices: useAppState, useAppDispatch, useUserPreferences, useFilters in src/hooks/\n9. Wrap the app with AppProvider in main.tsx\n10. Write unit tests for reducers and custom hooks using mock providers\n\nAcceptance Criteria:\n- Global state is accessible from any component via custom hooks\n- State updates via dispatch trigger re-renders correctly\n- User preferences persist across browser sessions via localStorage\n- TypeScript provides full type safety for actions and state\n- Reducer tests cover all action types\n- No prop drilling required for global state access",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "API Service Layer, Common Components, and Error Handling",
            "description": "Set up Axios with interceptors for API communication, create typed API service classes for all backend endpoints, implement common reusable UI components, and add error boundaries and toast notifications.",
            "status": "pending",
            "dependencies": [],
            "details": "1. Install axios\n2. Create src/services/api.ts with Axios instance configured with:\n   - Base URL from environment variable (VITE_API_BASE_URL)\n   - Request interceptor for headers (Content-Type, correlation IDs)\n   - Response interceptor for error normalization\n   - Timeout configuration\n3. Create typed API service classes in src/services/:\n   - fightersService.ts (getFighters, getFighterById, getFighterStats)\n   - eventsService.ts (getEvents, getEventById)\n   - fightsService.ts (getFights, getFightById)\n   - analyticsService.ts (getStyleEvolution, getEnduranceData)\n   - predictionsService.ts (getPredictions)\n4. Create src/types/api.ts with request/response TypeScript interfaces matching backend Pydantic schemas, including:\n   - Fighter profile types (physical stats from fighter_tott)\n   - Fight result types (winner, method, round from fight_results)\n   - Event listing types (event_details joined with fight_results)\n5. Implement a custom useApi hook in src/hooks/useApi.ts for data fetching with loading/error/data states\n6. Create common components in src/components/common/:\n   - Button.tsx (variants: primary, secondary, ghost, danger; sizes: sm, md, lg)\n   - Input.tsx (with label, error state, helper text)\n   - Card.tsx (with header, body, footer slots)\n   - LoadingSpinner.tsx and LoadingSkeleton.tsx\n   - ErrorBoundary.tsx (class component with fallback UI)\n   - Toast.tsx and ToastContainer.tsx (integrated with app state)\n   - Badge.tsx, Modal.tsx, Tooltip.tsx\n   - DataCaveatNote.tsx (reusable banner/note component for displaying data availability caveats)\n7. Integrate ToastContainer in App.tsx layout\n8. Add toast dispatch actions to the store (subtask 4)\n9. Write component tests for all common components\n10. Ensure all components meet WCAG 2.1 AA accessibility standards (aria labels, keyboard navigation, focus management)\n\nAcceptance Criteria:\n- Axios instance correctly sends requests to the configured API base URL\n- All service methods return properly typed responses\n- Error interceptor normalizes API errors and triggers toast notifications\n- All common components render correctly across responsive breakpoints\n- ErrorBoundary catches and displays errors gracefully without crashing the app\n- Toast notifications appear and auto-dismiss correctly\n- DataCaveatNote component renders correctly and is reusable\n- Components pass accessibility checks with axe-core\n- Component unit tests achieve >80% coverage",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Recent Events View and Fighter/Database Lookup View",
            "description": "Implement the two non-ML data views: a Recent Events view showing the last ~10 UFC events with card results, and a Fighter/Database Lookup view with search and full fighter profiles including a data availability caveat for pre-2015 fight stats.",
            "status": "pending",
            "dependencies": [
              3,
              4,
              5
            ],
            "details": "1. Recent Events View (src/pages/RecentEventsPage.tsx):\n   - Fetch data via eventsService.getEvents() (GET /events), ordered by date DESC, limit ~10\n   - Display each event as a card showing: event name, date, location\n   - Within each event card, list fight results: winner name, method (KO/TKO, Submission, Decision, etc.), round\n   - Data sourced from event_details joined with fight_results ordered by date_proper DESC\n   - Show loading skeletons while fetching\n   - Handle empty state and error state gracefully\n   - Responsive layout: stacked cards on mobile, grid on desktop\n\n2. Fighter/Database Lookup View (src/pages/FighterLookupPage.tsx and src/pages/FighterDetailPage.tsx):\n   - Search bar (Input component) for fighter name with debounced query\n   - Fighter list results via fightersService.getFighters(query) (GET /fighters?search=...)\n   - Fighter profile page via fightersService.getFighterById(id) (GET /fighters/{id}):\n     a. Physical stats section: height, weight, reach, stance, date of birth (from fighter_tott)\n     b. Record summary: wins, losses, draws, no contests\n     c. Full fight history table: opponent, event, date, result, method, round\n     d. IMPORTANT: Display a prominent, accessible caveat note \u00e2\u20ac\u201d 'Detailed round stats available from 2015+' \u00e2\u20ac\u201d on all fighter profiles. Use the DataCaveatNote component from subtask 5. This is because fight_stats (round-by-round detail) is only reliably available from 2015 onwards; older fights may show results but lack granular stats.\n   - Link fighter search results to individual fighter profile pages (/fighters/:id)\n   - Show loading skeletons during data fetch\n   - Handle no-results and error states\n\n3. Feature components in src/components/features/:\n   - EventCard.tsx (event summary with fight results list)\n   - FightResultRow.tsx (single fight result: winner, method, round)\n   - FighterSearchBar.tsx (debounced search input)\n   - FighterProfileHeader.tsx (name, record, physical stats)\n   - FightHistoryTable.tsx (sortable table of fight history)\n\n4. Write tests:\n   - Unit tests for EventCard, FightResultRow, FighterSearchBar, FighterProfileHeader, FightHistoryTable\n   - Integration tests for RecentEventsPage and FighterLookupPage with mocked API responses\n   - Test that DataCaveatNote renders on all fighter profile pages\n\nAcceptance Criteria:\n- Recent Events page displays last ~10 events with fight results (winner, method, round)\n- Fighter search returns relevant results and links to profile pages\n- Fighter profile shows physical stats, record, and full fight history\n- 'Detailed round stats available from 2015+' caveat note is always visible on fighter profiles\n- Both views are fully responsive (mobile-first)\n- Loading and error states are handled gracefully\n- No ML logic is involved in either view\n- All components pass accessibility checks",
            "testStrategy": "1. Test RecentEventsPage renders event cards with correct winner/method/round data from mocked API\n2. Test FighterLookupPage search bar triggers API call with correct query params\n3. Test FighterDetailPage displays physical stats and fight history correctly\n4. Verify DataCaveatNote ('Detailed round stats available from 2015+') renders on every fighter profile\n5. Test responsive layout at all four breakpoints\n6. Test loading skeleton displays during fetch\n7. Test error state displays when API fails\n8. Test empty state when no fighters match search query\n9. Accessibility audit with axe-core on both pages"
          }
        ]
      },
      {
        "id": 8,
        "title": "Interactive Fight Predictor UI",
        "description": "Develop the interactive fight predictor interface with fighter selection, parameter sliders, real-time prediction updates, and visualization of results.",
        "details": "Build the core fight predictor UI components:\n\n1. Fighter selection interface:\n   - Searchable fighter dropdown\n   - Fighter cards with photos and basic stats\n   - Weight class filtering\n   - Recent/popular fighter quick select\n\n2. Interactive parameter sliders:\n   - Adjustable physical attributes (height, weight, reach)\n   - Performance metrics sliders\n   - Style attribute adjustments\n   - Real-time visual feedback on changes\n\n3. Prediction visualization:\n   - Win probability gauge/meter\n   - Method probability distribution chart\n   - Round prediction distribution\n   - Confidence indicators\n\n4. Similar fights comparison:\n   - Table of historical similar matchups\n   - Outcome summaries\n   - Key stat comparisons\n   - Video link integration (where available)\n\n5. Real-time updates:\n   - Debounced API calls on slider changes\n   - Loading states for predictions\n   - Error handling with retry options\n   - Optimistic UI updates\n\n6. Mobile optimization:\n   - Touch-friendly slider controls\n   - Collapsible sections\n   - Responsive visualizations\n\nImplement using React components with Tailwind CSS for styling. Use Recharts for standard visualizations and consider D3.js for custom interactive elements.",
        "testStrategy": "1. Test slider interactions and updates\n2. Verify API integration with mock data\n3. Test responsive layouts on different devices\n4. Validate accessibility of interactive elements\n5. Test error states and recovery\n6. Verify performance with React profiler\n7. User testing for intuitiveness of controls\n8. Test touch interactions on mobile devices",
        "priority": "high",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Style Evolution Timeline Analyzer",
        "description": "Implement the style evolution timeline analyzer that visualizes how fighting styles and finish rates have evolved throughout UFC history with filtering and trend analysis.",
        "details": "Create the style evolution timeline feature:\n\n1. Data processing:\n   - Aggregate fight data by era and weight class\n   - Calculate style metrics over time\n   - Identify significant trend changes\n   - Compute finish rate statistics\n\n2. Timeline visualization:\n   - D3.js interactive timeline\n   - Selectable metrics (KO rate, submission rate, etc.)\n   - Weight class filtering\n   - Era demarcation (Pre-Zuffa, Early UFC, Modern era, etc.)\n\n3. Trend analysis:\n   - Moving averages of key metrics\n   - Trend line overlays\n   - Statistical significance indicators\n   - Notable events marking (rule changes, etc.)\n\n4. Interactive elements:\n   - Zoom and pan controls\n   - Tooltips with detailed information\n   - Highlight specific fighters or events\n   - Comparison mode for multiple metrics\n\n5. Filtering capabilities:\n   - Weight class selector\n   - Date range picker\n   - Fighter type filters (champions, contenders, etc.)\n   - Style category filters\n\n6. Export and sharing:\n   - Image export of visualizations\n   - Shareable links with filter state\n   - Embed codes for analysts\n\nImplement responsive design to ensure visualizations work on different screen sizes, with simplified views for mobile devices.",
        "testStrategy": "1. Test data aggregation accuracy\n2. Verify visualization rendering\n3. Test interactive elements (zoom, pan, tooltips)\n4. Validate filtering functionality\n5. Test responsive behavior on different devices\n6. Verify export and sharing features\n7. Performance testing with large datasets\n8. User testing for intuitiveness and insights",
        "priority": "medium",
        "dependencies": [
          3,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Fighter Endurance & Pacing Dashboard",
        "description": "Develop the fighter endurance and pacing dashboard that analyzes round-by-round performance degradation and predicts cardio performance in hypothetical matchups.",
        "details": "Build the fighter endurance analysis dashboard:\n\n1. Round-by-round data analysis:\n   - Process historical round statistics\n   - Calculate performance degradation rates\n   - Identify pacing patterns\n   - Compare early vs. late round performance\n\n2. Endurance profile visualization:\n   - Line charts of key metrics by round\n   - Comparative analysis with division average\n   - Stamina indicators\n   - Historical trend analysis\n\n3. Performance prediction models:\n   - Implement time series models for round prediction\n   - Create hypothetical 5-round projections\n   - Model cardio impact on performance\n   - Predict late-round effectiveness\n\n4. Comparative analysis:\n   - Fighter vs. fighter endurance comparison\n   - Historical matchup analysis\n   - Style matchup impact on cardio\n   - Training camp/preparation effects\n\n5. Interactive elements:\n   - Round selector\n   - Metric toggles (strikes, accuracy, movement)\n   - Scenario adjustments (pace, fighting style)\n   - Fight length selector\n\n6. Mobile optimization:\n   - Simplified visualizations for small screens\n   - Touch-friendly controls\n   - Progressive disclosure of complex data\n\nImplement using a combination of Recharts for standard visualizations and D3.js for custom interactive elements. Ensure all data visualizations have appropriate legends and context.",
        "testStrategy": "1. Validate round-by-round calculations\n2. Test visualization accuracy with known data\n3. Verify prediction model outputs\n4. Test comparative analysis features\n5. Validate interactive elements\n6. Test responsive design on different devices\n7. Performance testing with multiple fighters\n8. User testing for insights and usability",
        "priority": "medium",
        "dependencies": [
          3,
          7
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-30T15:55:11.132Z",
      "updated": "2026-02-24T21:43:38.775Z",
      "description": "Tasks for master context"
    }
  }
}